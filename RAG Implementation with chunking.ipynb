{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eceba96-6cb2-4d38-8e25-2853e44d375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain RAG Exercise: Build a \"Chat with Your Document\" System\n",
    "# =================================================================\n",
    "# This notebook walks you through building a simple RAG system step-by-step\n",
    "\n",
    "# SETUP: Run this first to install required packages\n",
    "!pip install langchain langchain-community langchain-openai pypdf chromadb openai tiktoken\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Set your OpenAI API key (get from https://platform.openai.com/api-keys)\n",
    "# IMPORTANT: Never commit your API key to version control!\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "print(\"‚úÖ Setup complete! Libraries imported successfully.\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LOAD YOUR PDF DOCUMENT\n",
    "# ============================================================================\n",
    "# Exercise 1: Load a PDF file\n",
    "# Instructions: Place a PDF file in the same directory as this notebook\n",
    "# or provide the full path to your PDF\n",
    "\n",
    "# TODO: Replace 'sample_document.pdf' with your PDF filename\n",
    "pdf_path = \"sample_document.pdf\"\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages from the PDF\")\n",
    "print(f\"üìÑ First page preview:\\n{documents[0].page_content[:500]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: SPLIT DOCUMENTS INTO CHUNKS\n",
    "# ============================================================================\n",
    "# Exercise 2: Experiment with chunk sizes\n",
    "# Why chunking? LLMs have token limits, and smaller chunks improve retrieval accuracy\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # TODO: Try different values (500, 1000, 1500)\n",
    "    chunk_overlap=200,      # TODO: Try different overlaps (100, 200, 300)\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks\")\n",
    "print(f\"üìù First chunk preview:\\n{chunks[0].page_content[:300]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: CREATE EMBEDDINGS AND STORE IN VECTOR DATABASE\n",
    "# ============================================================================\n",
    "# Exercise 3: Understanding embeddings\n",
    "# Embeddings convert text into numerical vectors that capture meaning\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create vector store (using Chroma, a simple local database)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"  # Saves to disk\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector database created successfully!\")\n",
    "print(f\"üìä Total vectors stored: {vectorstore._collection.count()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: TEST SIMILARITY SEARCH\n",
    "# ============================================================================\n",
    "# Exercise 4: See how similarity search works\n",
    "# This is the \"retrieval\" part of RAG\n",
    "\n",
    "test_query = \"What is this document about?\"  # TODO: Change this question\n",
    "\n",
    "# Search for similar chunks\n",
    "similar_docs = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"\\nüîç Top 3 most relevant chunks for: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(doc.page_content[:300])\n",
    "    print(\"...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: BUILD THE Q&A SYSTEM\n",
    "# ============================================================================\n",
    "# Exercise 5: Create the complete RAG chain\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # TODO: Try \"gpt-4\" for better results\n",
    "    temperature=0,               # TODO: Experiment with 0.0 to 1.0\n",
    ")\n",
    "\n",
    "# Create a custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" means put all context into one prompt\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Q&A system ready!\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: INTERACTIVE Q&A\n",
    "# ============================================================================\n",
    "# Exercise 6: Ask questions about your document!\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"Helper function to ask questions and display results\"\"\"\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üí° Answer: {result['result']}\")\n",
    "    print(\"\\nüìö Sources used:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"\\nSource {i}:\")\n",
    "        print(doc.page_content[:200] + \"...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Example questions - TODO: Replace with questions relevant to YOUR document\n",
    "questions = [\n",
    "    \"What is the main topic of this document?\",\n",
    "    \"Can you summarize the key points?\",\n",
    "    \"What are the most important takeaways?\"\n",
    "]\n",
    "\n",
    "# Ask each question\n",
    "for question in questions:\n",
    "    ask_question(question)\n",
    "\n",
    "# ============================================================================\n",
    "# BONUS: INTERACTIVE MODE\n",
    "# ============================================================================\n",
    "# Exercise 7: Create an interactive chat loop\n",
    "\n",
    "def chat_with_document():\n",
    "    \"\"\"Interactive chat session\"\"\"\n",
    "    print(\"\\nü§ñ Chat with Your Document (type 'quit' to exit)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    while True:\n",
    "        user_question = input(\"\\nYour question: \").strip()\n",
    "        \n",
    "        if user_question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_question:\n",
    "            continue\n",
    "        \n",
    "        ask_question(user_question)\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# chat_with_document()\n",
    "\n",
    "# ============================================================================\n",
    "# STUDENT EXERCISES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìù STUDENT EXERCISES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. EXPERIMENT WITH CHUNK SIZE:\n",
    "   - Change chunk_size to 500, 1000, and 1500\n",
    "   - Ask the same question with each setting\n",
    "   - Which works best for your document?\n",
    "\n",
    "2. TRY DIFFERENT QUESTIONS:\n",
    "   - Write 5 questions about your document\n",
    "   - Ask questions that require:\n",
    "     a) Direct facts\n",
    "     b) Summaries\n",
    "     c) Comparisons\n",
    "   - Which types work best?\n",
    "\n",
    "3. MODIFY THE PROMPT:\n",
    "   - Change the prompt_template to be more specific\n",
    "   - Try: \"Answer like you're explaining to a 5-year-old\"\n",
    "   - Or: \"Answer in bullet points\"\n",
    "   - How does this change responses?\n",
    "\n",
    "4. EXPERIMENT WITH TEMPERATURE:\n",
    "   - Try temperature values: 0, 0.5, 1.0\n",
    "   - Temperature controls creativity/randomness\n",
    "   - Which is better for factual Q&A?\n",
    "\n",
    "5. TEST WITH DIFFERENT DOCUMENTS:\n",
    "   - Try a textbook chapter\n",
    "   - Try a research paper\n",
    "   - Try a news article\n",
    "   - Which type works best with RAG?\n",
    "\n",
    "6. ANALYZE RETRIEVAL:\n",
    "   - For each question, look at the source documents\n",
    "   - Are they actually relevant?\n",
    "   - Try increasing/decreasing k (number of chunks retrieved)\n",
    "\n",
    "7. ADVANCED: Add conversation memory\n",
    "   - Research LangChain's ConversationBufferMemory\n",
    "   - Implement it so the system remembers previous questions\n",
    "   - Test with follow-up questions\n",
    "\n",
    "8. CHALLENGE: Build a comparison tool\n",
    "   - Load TWO different PDFs\n",
    "   - Create separate vector stores\n",
    "   - Ask the same question to both\n",
    "   - Compare answers\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLEAN UP (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment to delete the vector database and start fresh\n",
    "# import shutil\n",
    "# shutil.rmtree(\"./chroma_db\")\n",
    "# print(\"üóëÔ∏è Vector database deleted\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook complete! Happy learning! üéì\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
