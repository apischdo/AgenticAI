{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8428075-be5f-461e-87f3-a0c6106eb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Install Dependencies ===\n",
    "# We pin to versions that behave well in most Anaconda setups.\n",
    "# If your environment already has some of these, pip will skip or upgrade as needed.\n",
    "\n",
    "# 1) Make sure pip is fresh\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# 2) Gradio: pick a known-good window (4.28 ‚â§ gradio < 4.40)\n",
    "%pip install \"gradio>=4.28,<4.40\" pillow numpy pandas\n",
    "\n",
    "# 3) PyTorch CPU wheels (works everywhere; you can swap for CUDA if you have a GPU)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# 4) Vision + agents stack\n",
    "%pip install \"transformers>=4.44.2\" timm\n",
    "\n",
    "# 5) CrewAI + Pydantic v2 (CrewAI uses Pydantic v2 under the hood)\n",
    "%pip install \"crewai==0.51.1\" \"pydantic>=2.8,<3\"\n",
    "\n",
    "# 6) LangChain wrappers for OpenAI-compatible servers (LM Studio, etc.)\n",
    "%pip install \"langchain>=0.2.14\" \"langchain-openai>=0.1.23\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39ed9d-3448-4ce9-82c8-327076596c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Config for LM Studio (local OpenAI-compatible server) ===\n",
    "# We set everything inline (no .env file), so students can see & modify easily.\n",
    "\n",
    "import os, json, requests\n",
    "\n",
    "# 1) Point to LM Studio‚Äôs local API server (default URL/port)\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:1234/v1\"\n",
    "\n",
    "# 2) LM Studio ignores the actual key value but requires the Authorization header to exist\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"lmstudio-key\"\n",
    "\n",
    "# 3) Recommended classroom model (easy to run on most laptops)\n",
    "# In LM Studio, download & run:  TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
    "# Then set the model name here to match what LM Studio exposes.\n",
    "MODEL = \"mistral-7b-instruct-v0.2\"\n",
    "\n",
    "print(\"‚úÖ Config loaded:\")\n",
    "print(\"  OPENAI_BASE_URL:\", os.environ[\"OPENAI_BASE_URL\"])\n",
    "print(\"  OPENAI_API_KEY :\", \"(set)\")\n",
    "print(\"  MODEL          :\", MODEL)\n",
    "\n",
    "# ---- Quick connectivity check (student-proof) ----\n",
    "try:\n",
    "    resp = requests.get(\n",
    "        f\"{os.environ['OPENAI_BASE_URL'].rstrip('/')}/models\",\n",
    "        headers={\"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"},\n",
    "        timeout=6\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    models = resp.json()\n",
    "    print(\"\\n‚úÖ LM Studio is reachable. Models endpoint responded.\")\n",
    "    # Show a short snippet (avoid massive output)\n",
    "    print(json.dumps(models, indent=2)[:800], \"...\\n\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ö†Ô∏è Could not reach LM Studio.\")\n",
    "    print(\"   ‚Ä¢ Is LM Studio running?\")\n",
    "    print(\"   ‚Ä¢ Is the API server ON (top right in LM Studio)?\")\n",
    "    print(\"   ‚Ä¢ Is a chat model loaded?\")\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f940a-e190-400d-969a-d10b7db96501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Core imports, a tiny nutrition DB, and helper utilities ===\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- A tiny nutrition database (per 100g). Great for teaching; easy to extend. ---\n",
    "NUTRITION_DB = {\n",
    "    \"salmon\": {\"calories\": 208, \"protein_g\": 20, \"fat_g\": 13, \"carbs_g\": 0, \"fiber_g\": 0},\n",
    "    \"asparagus\": {\"calories\": 20, \"protein_g\": 2.2, \"fat_g\": 0.1, \"carbs_g\": 3.9, \"fiber_g\": 2.1},\n",
    "    \"tomato\": {\"calories\": 18, \"protein_g\": 0.9, \"fat_g\": 0.2, \"carbs_g\": 3.9, \"fiber_g\": 1.2},\n",
    "    \"rice\": {\"calories\": 130, \"protein_g\": 2.7, \"fat_g\": 0.3, \"carbs_g\": 28, \"fiber_g\": 0.4},\n",
    "    \"pasta\": {\"calories\": 131, \"protein_g\": 5, \"fat_g\": 1.1, \"carbs_g\": 25, \"fiber_g\": 1.3},\n",
    "    \"avocado\": {\"calories\": 160, \"protein_g\": 2, \"fat_g\": 15, \"carbs_g\": 9, \"fiber_g\": 7},\n",
    "    \"egg\": {\"calories\": 155, \"protein_g\": 13, \"fat_g\": 11, \"carbs_g\": 1.1, \"fiber_g\": 0},\n",
    "    \"chicken_breast\": {\"calories\": 165, \"protein_g\": 31, \"fat_g\": 3.6, \"carbs_g\": 0, \"fiber_g\": 0},\n",
    "    \"beef_steak\": {\"calories\": 271, \"protein_g\": 25, \"fat_g\": 19, \"carbs_g\": 0, \"fiber_g\": 0},\n",
    "    \"broccoli\": {\"calories\": 34, \"protein_g\": 2.8, \"fat_g\": 0.4, \"carbs_g\": 7, \"fiber_g\": 2.6},\n",
    "    \"potato\": {\"calories\": 77, \"protein_g\": 2, \"fat_g\": 0.1, \"carbs_g\": 17, \"fiber_g\": 2.2},\n",
    "    \"spinach\": {\"calories\": 23, \"protein_g\": 2.9, \"fat_g\": 0.4, \"carbs_g\": 3.6, \"fiber_g\": 2.2},\n",
    "    \"lemon\": {\"calories\": 29, \"protein_g\": 1.1, \"fat_g\": 0.3, \"carbs_g\": 9.3, \"fiber_g\": 2.8},\n",
    "    \"olive_oil\": {\"calories\": 884, \"protein_g\": 0, \"fat_g\": 100, \"carbs_g\": 0, \"fiber_g\": 0},\n",
    "}\n",
    "\n",
    "# Candidate labels for zero-shot recognition\n",
    "CANDIDATE_LABELS = [\n",
    "    \"salmon\", \"asparagus\", \"tomato\", \"rice\", \"pasta\", \"avocado\", \"egg\",\n",
    "    \"chicken breast\", \"beef steak\", \"broccoli\", \"potato\", \"spinach\", \"lemon\",\n",
    "    \"olive oil\", \"cheeseburger\", \"pizza\", \"pancakes\", \"sushi\", \"tofu\"\n",
    "]\n",
    "\n",
    "def to_safe_key(label: str) -> str:\n",
    "    \"\"\"Normalize model labels to our DB keys (simple mapping demo).\"\"\"\n",
    "    label = label.lower().strip().replace(\" \", \"_\")\n",
    "    mapping = {\n",
    "        \"chicken_breast\": \"chicken_breast\",\n",
    "        \"chicken\": \"chicken_breast\",\n",
    "        \"beef_steak\": \"beef_steak\",\n",
    "        \"steak\": \"beef_steak\",\n",
    "    }\n",
    "    return mapping.get(label, label)\n",
    "\n",
    "def scale_nutrition(per100g: Dict[str, float], grams: float) -> Dict[str, float]:\n",
    "    \"\"\"Scale per-100g nutrition to any gram amount.\"\"\"\n",
    "    factor = grams / 100.0\n",
    "    return {k: round(v * factor, 2) for k, v in per100g.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9e93f-0d75-421b-aa7f-e547820250a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Load CLIP + zero-shot label ranking ===\n",
    "# We pick CLIP because it's small, fast to download, and works well for label matching.\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "DEVICE = \"cpu\"  # set to \"cuda\" if you installed GPU torch and have a GPU\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "\n",
    "def clip_rank_labels(pil_image: Image.Image, candidate_texts: List[str], top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Compare an image against candidate labels and return the top_k by probability.\n",
    "    This is true zero-shot classification using CLIP's image-text similarity.\n",
    "    \"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=[f\"a photo of {t}\" for t in candidate_texts],\n",
    "        images=pil_image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image  # shape (1, num_texts)\n",
    "        probs = logits_per_image.softmax(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "    ranked = sorted(zip(candidate_texts, probs), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{\"label\": lab, \"prob\": float(p)} for lab, p in ranked]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0661c899-f2b7-4d9d-838d-8e6ff18b5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: LLM + CrewAI Agents ===\n",
    "# CrewAI expects each Agent to have an LLM object that supports `.bind()` (LangChain models do).\n",
    "# We use LangChain's ChatOpenAI pointing at LM Studio (OpenAI-compatible /chat/completions).\n",
    "\n",
    "import os\n",
    "from crewai import Agent  # We're using Agents to organize roles (lightweight for this project)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a LangChain chat LLM backed by LM Studio\n",
    "chat_llm = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL\", \"mistral-7b-instruct-v0.2\"),   # must match LM Studio's model name\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"lmstudio-key\"),    # any non-empty string\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\", \"http://localhost:1234/v1\"),\n",
    "    temperature=0.3,\n",
    "    max_tokens=700,\n",
    ")\n",
    "\n",
    "# Utility: call the chat LLM with a system + user prompt and return content text\n",
    "def llm_chat(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    LM Studio compatibility: many local prompt templates only accept `user` & `assistant`.\n",
    "    So we fold the \"system\" guidance into a single `user` message to avoid 400s.\n",
    "\n",
    "    If you later switch to a server/model that supports `system` messages,\n",
    "    you can toggle USE_SYSTEM=True below to use the true system+user format.\n",
    "    \"\"\"\n",
    "    USE_SYSTEM = False  # Keep False for LM Studio templates that 400 on 'system'\n",
    "\n",
    "    if USE_SYSTEM:\n",
    "        # Native format (works with real OpenAI / some templates)\n",
    "        msg = chat_llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt}\n",
    "        ])\n",
    "    else:\n",
    "        # Compatibility format: one user message that embeds the system guidance.\n",
    "        one_message = (\n",
    "            \"### System instruction (follow strictly):\\n\"\n",
    "            f\"{system_prompt}\\n\\n\"\n",
    "            \"### User request:\\n\"\n",
    "            f\"{user_prompt}\"\n",
    "        )\n",
    "        # Passing a plain string creates a single HumanMessage under the hood.\n",
    "        msg = chat_llm.invoke(one_message)\n",
    "\n",
    "    return getattr(msg, \"content\", str(msg))\n",
    "\n",
    "# ---- Define agents (roles) so students see the multi-agent structure ----\n",
    "vision_analyst = Agent(\n",
    "    role=\"Vision Analyst\",\n",
    "    goal=\"Identify likely foods in a photo using a vision tool and report top-3 with confidence.\",\n",
    "    backstory=\"A careful computer-vision assistant focused on accurate label suggestions.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=chat_llm\n",
    ")\n",
    "\n",
    "nutritionist = Agent(\n",
    "    role=\"Nutritionist\",\n",
    "    goal=\"Estimate calories/macros from ingredient list + portions using a small nutrition DB.\",\n",
    "    backstory=\"A data-driven nutrition expert who is transparent about approximations.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=chat_llm\n",
    ")\n",
    "\n",
    "recipe_planner = Agent(\n",
    "    role=\"Recipe Planner\",\n",
    "    goal=\"Create a simple, tasty, personalized recipe that matches dietary goals.\",\n",
    "    backstory=\"A friendly home chef who writes clear steps and practical substitutions.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=chat_llm\n",
    ")\n",
    "\n",
    "# ---- Agent tools as plain Python functions ----\n",
    "def tool_detect_foods(image: Image.Image, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Use CLIP zero-shot to propose top_k food labels with probabilities.\"\"\"\n",
    "    return clip_rank_labels(image, CANDIDATE_LABELS, top_k=top_k)\n",
    "\n",
    "def tool_estimate_nutrition(detections: List[Dict[str, Any]], grams_map: Dict[str, float]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert detected labels + gram estimates into per-ingredient + total macro counts.\n",
    "    grams_map keys can be 'salmon', 'beef steak', etc.; we normalize to DB keys.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    totals = {\"calories\":0, \"protein_g\":0, \"fat_g\":0, \"carbs_g\":0, \"fiber_g\":0}\n",
    "\n",
    "    for d in detections:\n",
    "        raw_label = d[\"label\"]\n",
    "        key = to_safe_key(raw_label)\n",
    "        grams = float(grams_map.get(raw_label, grams_map.get(key, 0)))\n",
    "        if grams <= 0:\n",
    "            continue\n",
    "        if key not in NUTRITION_DB:\n",
    "            # Unknown items are skipped (students can extend DB as exercise)\n",
    "            continue\n",
    "        scaled = scale_nutrition(NUTRITION_DB[key], grams)\n",
    "        items.append({\"ingredient\": raw_label, \"grams\": grams, **scaled})\n",
    "        for k in totals:\n",
    "            totals[k] += scaled[k]\n",
    "\n",
    "    totals = {k: round(v, 2) for k, v in totals.items()}\n",
    "    return {\"items\": items, \"totals\": totals}\n",
    "\n",
    "# ---- Orchestrator that \"coordinates\" agents (simple, explicit for teaching) ----\n",
    "def coordinator(image: Image.Image,\n",
    "                user_prefs: str,\n",
    "                servings: int = 2,\n",
    "                calories_goal: int = 600,\n",
    "                grams_overrides: Dict[str, float] | None = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    High-level pipeline:\n",
    "      1) Vision Analyst -> detect labels (via tool_detect_foods)\n",
    "      2) Nutritionist   -> estimate macros (via tool_estimate_nutrition)\n",
    "      3) Recipe Planner -> generate recipe text (via LLM)\n",
    "    We keep this orchestrator explicit for clarity in a beginner notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Vision\n",
    "    detections = tool_detect_foods(image, top_k=3)\n",
    "    labels = [d[\"label\"] for d in detections]\n",
    "\n",
    "    # 2) Portion defaults (students can override in UI)\n",
    "    default_grams = {\n",
    "        lab: 150.0 if any(k in lab for k in [\"salmon\", \"steak\", \"chicken\"]) else 80.0\n",
    "        for lab in labels\n",
    "    }\n",
    "    grams_map = default_grams | (grams_overrides or {})\n",
    "\n",
    "    # 3) Nutrition\n",
    "    nutrition = tool_estimate_nutrition(detections, grams_map)\n",
    "\n",
    "    # 4) Recipe (LLM)\n",
    "    system = \"You are a professional chef and nutrition coach. Be concise, friendly, and practical.\"\n",
    "    user = f\"\"\"\n",
    "Detected ingredients (top-3): {labels}\n",
    "Estimated totals (approx, for the whole dish): {nutrition['totals']}\n",
    "User preferences/restrictions: {user_prefs}\n",
    "Target calories per serving: ~{calories_goal}\n",
    "Servings: {servings}\n",
    "\n",
    "Write:\n",
    "1) A brief 2-sentence assessment of the plate and nutrition.\n",
    "2) A personalized recipe (ingredients list in grams + step-by-step instructions).\n",
    "3) One tip to adjust calories up or down while keeping protein reasonable.\n",
    "\"\"\"\n",
    "    recipe_text = llm_chat(system, user)\n",
    "\n",
    "    return {\"detections\": detections, \"nutrition\": nutrition, \"recipe_text\": recipe_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271eceda-6ddc-45b9-a87f-9d77bdb6abea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Cell 6: Gradio UI ===\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "def nutrition_df(nutrition: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Render nutrition dict as a simple table with a TOTAL row.\"\"\"\n",
    "    items = nutrition.get(\"items\", [])\n",
    "    if not items:\n",
    "        return pd.DataFrame([nutrition[\"totals\"]])\n",
    "    df = pd.DataFrame(items)\n",
    "    total_row = {\"ingredient\": \"TOTAL\", \"grams\": df[\"grams\"].sum(), **nutrition[\"totals\"]}\n",
    "    return pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "def run_pipeline(image, servings, calories_goal, dietary_prefs, grams_overrides_json):\n",
    "    # Convert incoming numpy array to PIL image\n",
    "    pil = Image.fromarray(image)\n",
    "\n",
    "    # Try to parse JSON overrides (students can specify {\"salmon\":180, \"asparagus\":90})\n",
    "    overrides = {}\n",
    "    if grams_overrides_json:\n",
    "        try:\n",
    "            overrides = json.loads(grams_overrides_json)\n",
    "        except Exception:\n",
    "            overrides = {}\n",
    "\n",
    "    # Orchestrate\n",
    "    result = coordinator(\n",
    "        pil,\n",
    "        user_prefs=dietary_prefs or \"\",\n",
    "        servings=int(servings),\n",
    "        calories_goal=int(calories_goal),\n",
    "        grams_overrides=overrides or None\n",
    "    )\n",
    "\n",
    "    labels = result[\"detections\"]\n",
    "    df = nutrition_df(result[\"nutrition\"])\n",
    "    recipe = result[\"recipe_text\"]\n",
    "\n",
    "    return labels, df, recipe\n",
    "\n",
    "with gr.Blocks(title=\"üçΩÔ∏è Agentic Food Analyzer\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## üçΩÔ∏è Agentic Food Analyzer\\nUpload a food photo ‚Üí get nutrition estimates ‚Üí receive a personalized recipe.\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            image_in = gr.Image(type=\"numpy\", label=\"Upload a food photo\", height=320)\n",
    "            servings = gr.Slider(1, 6, value=2, step=1, label=\"Servings\")\n",
    "            calories_goal = gr.Slider(200, 1200, value=600, step=50, label=\"Target Calories per Serving\")\n",
    "            dietary_prefs = gr.Textbox(label=\"Dietary preferences / restrictions (e.g., high protein, no dairy, gluten-free)\")\n",
    "            grams_overrides = gr.Textbox(\n",
    "                label=\"(Optional) Portion overrides as JSON (e.g., {\\\"salmon\\\": 180, \\\"asparagus\\\": 90})\",\n",
    "                placeholder='{\"salmon\": 180, \"asparagus\": 90}'\n",
    "            )\n",
    "            run_btn = gr.Button(\"Analyze & Create Recipe\")\n",
    "        with gr.Column(scale=1):\n",
    "            labels_out = gr.JSON(label=\"Top-3 detected foods (label + probability)\")\n",
    "            nutrition_out = gr.Dataframe(label=\"Estimated Nutrition (approx)\", wrap=True)\n",
    "            recipe_out = gr.Markdown(label=\"Personalized Recipe\")\n",
    "\n",
    "    run_btn.click(\n",
    "        run_pipeline,\n",
    "        inputs=[image_in, servings, calories_goal, dietary_prefs, grams_overrides],\n",
    "        outputs=[labels_out, nutrition_out, recipe_out]\n",
    "    )\n",
    "\n",
    "demo.launch(share=False)  # set share=True if you want a public link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336a18a-f208-495b-a457-c0ce6eada217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7 (Optional): Quick local test without opening the UI ===\n",
    "# Uses the image you uploaded in this chat environment. On your machine,\n",
    "# replace the path with your local image file to sanity-check the pipeline.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "sample_path = Path(\"/Users/armenpischdotchian/Desktop/food.png\")  # change if needed\n",
    "if sample_path.exists():\n",
    "    img = Image.open(sample_path).convert(\"RGB\")\n",
    "    display(img)\n",
    "    result = coordinator(img, user_prefs=\"high protein, no dairy\", servings=2, calories_goal=600)\n",
    "    print(\"Detections:\", result[\"detections\"])\n",
    "    display(nutrition_df(result[\"nutrition\"]))\n",
    "    print(result[\"recipe_text\"])\n",
    "else:\n",
    "    print(\"Sample image not found at:\", sample_path)\n",
    "    print(\"Tip: set 'sample_path' to a local image to test quickly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d28b9-2bd3-429e-8382-9c88d9fe3bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
