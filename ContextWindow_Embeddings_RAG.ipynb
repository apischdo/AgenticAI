{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00bacc6b-931e-4398-886f-6f0a0a041193",
   "metadata": {},
   "source": [
    "# Lab: Chunking, Embeddings, and RAG with LlamaIndex + Ollama (Local Only)\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "- Take a **long-ish text** and **chunk** it into smaller pieces.\n",
    "- Use an **embedding model** to represent those chunks as vectors.\n",
    "- Build a simple **RAG (Retrieval-Augmented Generation)** pipeline:\n",
    "  - Retrieve the most relevant chunks using embeddings.\n",
    "  - Let a local LLM (via **Ollama**) generate an answer based on those chunks.\n",
    "\n",
    "Everything runs **locally** using:\n",
    "\n",
    "- **Ollama** for the LLM and (optionally) the embedding model.\n",
    "- **LlamaIndex** for chunking, indexing, and querying.\n",
    "\n",
    "By the end, you should be able to:\n",
    "\n",
    "- Explain what **chunking** is and why it helps with context windows.\n",
    "- Explain what an **embedding** is and how it‚Äôs used for retrieval.\n",
    "- Understand how RAG adds **external knowledge** to an LLM.\n",
    "\n",
    "Before you run this notebook, ensure the following pre-requisites have met:\n",
    "Ollama installed (on each machine)\n",
    "1) Download from: https://ollama.com\n",
    "2) Double-click and invoke Ollama and ask a question to force it to download the required packages\n",
    "3) After install, in a terminal or command  window, run:\n",
    "\n",
    "    ollama pull llama3\n",
    "    ollama pull nomic-embed-text\n",
    "\n",
    "Then install Conda environment and Python packages:\n",
    "\n",
    "    conda create -n rag_ollama python=3.11 -y\n",
    "    conda activate rag_ollama\n",
    "\n",
    "    pip install \\\n",
    "      llama-index \\\n",
    "      llama-index-llms-ollama \\\n",
    "      llama-index-embeddings-ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513227c-49c1-49ed-b91d-6371793e6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-core llama-index-llms-ollama llama-index-embeddings-ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626a80e-fd52-4d74-a8f6-482d7eeb2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define a sample \"document\" and a user question\n",
    "\n",
    "# This is our \"long\" document. In real scenarios, this might be\n",
    "# several pages of a PDF or a long article.\n",
    "doc_text = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a technique that combines large language models (LLMs)\n",
    "with an external knowledge source. Instead of relying only on what was seen during training,\n",
    "the model can look up relevant information at query time. This helps reduce hallucinations\n",
    "and allows the model to stay up to date with new information.\n",
    "\n",
    "A typical RAG pipeline works in two main steps. First, a retrieval component searches a\n",
    "knowledge base to find the most relevant pieces of information, often called documents or chunks.\n",
    "Second, the language model reads both the user question and the retrieved chunks, and then\n",
    "generates an answer that is grounded in those sources.\n",
    "\n",
    "To make retrieval efficient and accurate, we usually convert text into embeddings, which are\n",
    "vector representations of meaning. Texts with similar meaning end up with similar embeddings.\n",
    "We then store these embeddings in a vector database or index. At query time, we embed the user\n",
    "question and look for the most similar vectors. Those corresponding chunks are fed into the LLM\n",
    "as context.\n",
    "\n",
    "Chunking is necessary because long documents cannot fit entirely into the model's context window.\n",
    "By breaking documents into smaller chunks, we can efficiently search and select only the most\n",
    "relevant parts. Chunk size and overlap are design choices: too small and we lose context; too large\n",
    "and we may hit context limits or retrieve irrelevant material.\n",
    "\"\"\"\n",
    "\n",
    "# A user question we want the system to answer.\n",
    "user_query = \"Why do we need chunking in a RAG system, and how is it related to the context window?\"\n",
    "\n",
    "print(\"üìÑ Document preview (first 500 characters):\\n\")\n",
    "print(doc_text[:500], \"...\\n\")\n",
    "\n",
    "print(\"‚ùì User query:\")\n",
    "print(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b83fbc-8606-476f-9fe5-ac38b9566711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Chunk the document into smaller pieces\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "# Simple manual chunking just for teaching:\n",
    "# We'll break the document into chunks of about N characters.\n",
    "# In real systems, we often chunk by tokens or sentences.\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "def simple_char_chunk(text, chunk_size=400, overlap=50):\n",
    "    \"\"\"\n",
    "    Very simple character-based chunking.\n",
    "    Not production-ready, but good for teaching.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start = end - overlap  # step with overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = simple_char_chunk(doc_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "print(f\"üîπ Number of chunks created: {len(chunks)}\\n\")\n",
    "for i, ch in enumerate(chunks, start=1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(ch[:250], \"...\\n\")  # show only first 250 chars for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1a051-cc07-4fe7-8417-f79234651854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configure LlamaIndex to use Ollama locally (no OpenAI)\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Make sure the Ollama server is running in the background (ollama serve).\n",
    "# Also ensure you have pulled the necessary models:\n",
    "#   ollama pull llama3\n",
    "#   ollama pull nomic-embed-text\n",
    "\n",
    "# Set the LLM to use Ollama's \"llama3\" model\n",
    "Settings.llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Set the embedding model to use Ollama's \"nomic-embed-text\" model\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "print(\"‚úÖ LlamaIndex is now configured to use Ollama for both LLM and embeddings (locally).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf57b4-f315-41bf-af29-cf55ca999e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build a vector index from the chunks and run a RAG-style query\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "# Wrap each chunk as a LlamaIndex Document.\n",
    "# Note: Document expects keyword arguments (text=...).\n",
    "documents = [Document(text=chunk) for chunk in chunks]\n",
    "\n",
    "# Build a vector index from the documents.\n",
    "# Under the hood:\n",
    "# - Each chunk is embedded via OllamaEmbedding.\n",
    "# - The embeddings are stored in a vector index.\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a query engine.\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Ask our user query.\n",
    "response = query_engine.query(user_query)\n",
    "\n",
    "print(\"‚ùì Query:\")\n",
    "print(user_query)\n",
    "print(\"\\nüß† RAG-style answer (LlamaIndex + Ollama):\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3f516-dce4-4efa-b5a6-775520f5fe5b",
   "metadata": {},
   "source": [
    "## Student Exercises\n",
    "\n",
    "### Exercise 1 ‚Äì Change the Document\n",
    "\n",
    "1. Go back to **Cell 2** and replace `doc_text` with your own text, for example:\n",
    "   - A section from a textbook,\n",
    "   - A class reading,\n",
    "   - A web article (copied as plain text).\n",
    "2. Keep the rest of the notebook the same.\n",
    "3. Rerun **Cell 2 ‚Üí 3 ‚Üí 4 ‚Üí 5**.\n",
    "\n",
    "**Questions:**\n",
    "- How many chunks are created now?\n",
    "- Does the answer from the RAG pipeline correctly reflect the content of your new document?\n",
    "\n",
    "\n",
    "### Exercise 2 ‚Äì Experiment with Chunk Size\n",
    "\n",
    "1. In **Cell 3**, change `CHUNK_SIZE` and `CHUNK_OVERLAP`, for example:\n",
    "   - Small chunks: `CHUNK_SIZE = 200`, `CHUNK_OVERLAP = 20`\n",
    "   - Larger chunks: `CHUNK_SIZE = 800`, `CHUNK_OVERLAP = 100`\n",
    "2. Rerun **Cell 3 ‚Üí 5**.\n",
    "\n",
    "**Questions:**\n",
    "- Do smaller chunks make the answer more specific or more fragmented?\n",
    "- Do larger chunks risk including irrelevant parts of the text?\n",
    "- How might this relate to the **context window** of the LLM?\n",
    "\n",
    "\n",
    "### Exercise 3 ‚Äì Ask Different Questions\n",
    "\n",
    "1. In **Cell 2**, keep the same `doc_text` but change `user_query` to questions like:\n",
    "   - \"What is an embedding and why is it useful?\"\n",
    "   - \"How does RAG reduce hallucinations in language models?\"\n",
    "   - \"What is the role of chunking in this system?\"\n",
    "2. Rerun **Cell 2 ‚Üí 5**.\n",
    "\n",
    "**Questions:**\n",
    "- Does the model‚Äôs answer stay grounded in the text?\n",
    "- Can you find a question where the model starts to guess beyond the given text?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
